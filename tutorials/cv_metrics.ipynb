{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GT Boxes\n",
    "gt_boxes= {\"img_00285.png\": [[480, 457, 515, 529], [637, 435, 676, 536]]}\n",
    "#Pred Boxes\n",
    "pred_boxs={\"img_00285.png\": {\"boxes\": [[330, 463, 387, 505], [356, 456, 391, 521], [420, 433, 451, 498], [328, 465, 403, 540], [480, 477, 508, 522], [357, 460, 417, 537], [344, 459, 389, 493], [485, 459, 503, 511], [336, 463, 362, 496], [468, 435, 520, 521], [357, 458, 382, 485], [649, 479, 670, 531], [484, 455, 514, 519], [641, 439, 670, 532]], \"scores\": [0.0739, 0.0843, 0.091, 0.1008, 0.1012, 0.1058, 0.1243, 0.1266, 0.1342, 0.1618, 0.2452, 0.8505, 0.9113, 0.972]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(pred_boxes):\n",
    "    \"\"\"Creates a dictionary of from model_scores to image ids.\n",
    "    Args:\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
    "    Returns:\n",
    "        dict: keys are model_scores and values are image ids (usually filenames)\n",
    "    \"\"\"\n",
    "    model_score={}\n",
    "    for img_id, val in pred_boxes.items():\n",
    "        for score in val['scores']:\n",
    "            if score not in model_score.keys():\n",
    "                model_score[score]=[img_id]\n",
    "            else:\n",
    "                model_score[score].append(img_id)\n",
    "    return model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou( gt_bbox, pred_bbox):\n",
    "    '''\n",
    "    This function takes the predicted bounding box and ground truth bounding box and \n",
    "    return the IoU ratio\n",
    "    '''\n",
    "    x_topleft_gt, y_topleft_gt, x_bottomright_gt, y_bottomright_gt= gt_bbox\n",
    "    x_topleft_p, y_topleft_p, x_bottomright_p, y_bottomright_p= pred_bbox\n",
    "    \n",
    "    if (x_topleft_gt > x_bottomright_gt) or (y_topleft_gt> y_bottomright_gt):\n",
    "        raise AssertionError(\"Ground Truth Bounding Box is not correct\")\n",
    "    if (x_topleft_p > x_bottomright_p) or (y_topleft_p> y_bottomright_p):\n",
    "        raise AssertionError(\"Predicted Bounding Box is not correct\",x_topleft_p, x_bottomright_p,y_topleft_p,y_bottomright_gt)\n",
    "        \n",
    "         \n",
    "    #if the GT bbox and predcited BBox do not overlap then iou=0\n",
    "    if(x_bottomright_gt< x_topleft_p):\n",
    "        # If bottom right of x-coordinate  GT  bbox is less than or above the top left of x coordinate of  the predicted BBox\n",
    "        \n",
    "        return 0.0\n",
    "    if(y_bottomright_gt< y_topleft_p):  # If bottom right of y-coordinate  GT  bbox is less than or above the top left of y coordinate of  the predicted BBox\n",
    "        \n",
    "        return 0.0\n",
    "    if(x_topleft_gt> x_bottomright_p): # If bottom right of x-coordinate  GT  bbox is greater than or below the bottom right  of x coordinate of  the predcited BBox\n",
    "        \n",
    "        return 0.0\n",
    "    if(y_topleft_gt> y_bottomright_p): # If bottom right of y-coordinate  GT  bbox is greater than or below the bottom right  of y coordinate of  the predcited BBox\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    \n",
    "    GT_bbox_area = (x_bottomright_gt -  x_topleft_gt + 1) * (  y_bottomright_gt -y_topleft_gt + 1)\n",
    "    Pred_bbox_area =(x_bottomright_p - x_topleft_p + 1 ) * ( y_bottomright_p -y_topleft_p + 1)\n",
    "    \n",
    "    x_top_left =np.max([x_topleft_gt, x_topleft_p])\n",
    "    y_top_left = np.max([y_topleft_gt, y_topleft_p])\n",
    "    x_bottom_right = np.min([x_bottomright_gt, x_bottomright_p])\n",
    "    y_bottom_right = np.min([y_bottomright_gt, y_bottomright_p])\n",
    "    \n",
    "    intersection_area = (x_bottom_right- x_top_left + 1) * (y_bottom_right-y_top_left  + 1)\n",
    "    \n",
    "    union_area = (GT_bbox_area + Pred_bbox_area - intersection_area)\n",
    "   \n",
    "    return intersection_area/union_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_recall(image_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "    Args:\n",
    "        img_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "    Returns:\n",
    "        tuple: of floats of (precision, recall)\n",
    "    \"\"\"\n",
    "    true_positive=0\n",
    "    false_positive=0\n",
    "    false_negative=0\n",
    "    for img_id, res in image_results.items():\n",
    "        true_positive +=res['true_positive']\n",
    "        false_positive += res['false_positive']\n",
    "        false_negative += res['false_negative']\n",
    "        try:\n",
    "            precision = true_positive/(true_positive+ false_positive)\n",
    "        except ZeroDivisionError:\n",
    "            precision=0.0\n",
    "        try:\n",
    "            recall = true_positive/(true_positive + false_negative)\n",
    "        except ZeroDivisionError:\n",
    "            recall=0.0\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "    all_pred_indices= range(len(pred_boxes))\n",
    "    all_gt_indices=range(len(gt_boxes))\n",
    "    if len(all_pred_indices)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    if len(all_gt_indices)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    \n",
    "    gt_idx_thr=[]\n",
    "    pred_idx_thr=[]\n",
    "    ious=[]\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou= calc_iou(gt_box, pred_box)\n",
    "            \n",
    "            if iou >iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "    iou_sort = np.argsort(ious)[::1]\n",
    "    if len(iou_sort)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    else:\n",
    "        gt_match_idx=[]\n",
    "        pred_match_idx=[]\n",
    "        for idx in iou_sort:\n",
    "            gt_idx=gt_idx_thr[idx]\n",
    "            pr_idx= pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if(gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp= len(gt_match_idx)\n",
    "        fp= len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "    return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_avg_precision_at_iou(gt_boxes, pred_bb, iou_thr=0.5):\n",
    "    \n",
    "    model_scores = get_model_scores(pred_bb)\n",
    "    sorted_model_scores= sorted(model_scores.keys())\n",
    "# Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "    for img_id in pred_bb.keys():\n",
    "        \n",
    "        arg_sort = np.argsort(pred_bb[img_id]['scores'])\n",
    "        pred_bb[img_id]['scores'] = np.array(pred_bb[img_id]['scores'])[arg_sort].tolist()\n",
    "        pred_bb[img_id]['boxes'] = np.array(pred_bb[img_id]['boxes'])[arg_sort].tolist()\n",
    "pred_boxes_pruned = deepcopy(pred_bb)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    model_thrs = []\n",
    "    img_results = {}\n",
    "# Loop over model score thresholds and calculate precision, recall\n",
    "    for ithr, model_score_thr in enumerate(sorted_model_scores[:-1]):\n",
    "            # On first iteration, define img_results for the first time:\n",
    "        print(\"Mode score : \", model_score_thr)\n",
    "        img_ids = gt_boxes.keys() if ithr == 0 else model_scores[model_score_thr]\n",
    "for img_id in img_ids:\n",
    "               \n",
    "            gt_boxes_img = gt_boxes[img_id]\n",
    "            box_scores = pred_boxes_pruned[img_id]['scores']\n",
    "            start_idx = 0\n",
    "            for score in box_scores:\n",
    "                if score <= model_score_thr:\n",
    "                    pred_boxes_pruned[img_id]\n",
    "                    start_idx += 1\n",
    "                else:\n",
    "                    break \n",
    "            # Remove boxes, scores of lower than threshold scores:\n",
    "            pred_boxes_pruned[img_id]['scores']= pred_boxes_pruned[img_id]['scores'][start_idx:]\n",
    "            pred_boxes_pruned[img_id]['boxes']= pred_boxes_pruned[img_id]['boxes'][start_idx:]\n",
    "# Recalculate image results for this image\n",
    "            print(img_id)\n",
    "            img_results[img_id] = get_single_image_results(gt_boxes_img, pred_boxes_pruned[img_id]['boxes'], iou_thr=0.5)\n",
    "# calculate precision and recall\n",
    "        prec, rec = calc_precision_recall(img_results)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        model_thrs.append(model_score_thr)\n",
    "precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    prec_at_rec = []\n",
    "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args= np.argwhere(recalls>recall_level).flatten()\n",
    "            prec= max(precisions[args])\n",
    "            print(recalls,\"Recall\")\n",
    "            print(      recall_level,\"Recall Level\")\n",
    "            print(       args, \"Args\")\n",
    "            print(       prec, \"precision\")\n",
    "        except ValueError:\n",
    "            prec=0.0\n",
    "        prec_at_rec.append(prec)\n",
    "    avg_prec = np.mean(prec_at_rec) \n",
    "    return {\n",
    "        'avg_prec': avg_prec,\n",
    "        'precisions': precisions,\n",
    "        'recalls': recalls,\n",
    "        'model_thrs': model_thrs}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc891b44e2bbb06a35a4c61455e5cf92544f72af2b4fd8b0b73aa428f4fc8fe9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
